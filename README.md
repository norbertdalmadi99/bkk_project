# ğŸš BKK Realtime Data Pipeline (GCP + PostgreSQL)

A production-style data ingestion pipeline that collects realtime public transport data and static GTFS feeds using Python, Bash and Google Cloud.

---

## ğŸ§± System Architecture

BKK API â†’ Python ingestion â†’ GCP Compute Engine (cron jobs) â†’ Cloud SQL PostgreSQL

Key components:

- Realtime vehicle ingestion (minutely)
- Static GTFS refresh (daily)
- GitHub auto-deployment via git auto pull
- Lock-safe cron execution

![Architecture](docs/architecture.png)

## ğŸ—„ Database ER Diagram
![ER Diagram](docs/data_model.png)

---

## âš™ï¸ Tech Stack

- Python
- Bash
- PostgreSQL (Cloud SQL)
- Google Cloud Compute Engine
- Cron scheduling
- GitHub

---

## ğŸ”„ Data Pipelines

### 1ï¸âƒ£ Realtime Vehicle Pipeline

Runs every minute.

Steps:

1. Activate Python virtual environment
2. Load environment variables from `.env`
3. Fetch GTFS-RT vehicle positions
4. Insert into PostgreSQL

Safety features:

- `flock` locking
- `set -euo pipefail`
- structured logging

---

### 2ï¸âƒ£ Static GTFS Pipeline

Runs daily at 03:00.

Processes:

- stops
- routes
- trips
- shapes

---

### 3ï¸âƒ£ Git Auto Deployment

Hourly cron job that pulls latest commits from GitHub.

Prevents:

- merge conflicts
- duplicate runs

---

## ğŸ§  Design Decisions

Why Compute Engine instead of Cloud Run?

- Persistent scheduling
- Simple cron-based orchestration
- Easier debugging during development

Why Bash entrypoints?

- Clear separation of environment and logic
- Production-like workflow

---

## ğŸ›  Challenges & Solutions

- Cron used /bin/sh instead of bash â†’ fixed by explicit /bin/bash
- Concurrent cron runs â†’ solved with flock locking
- Cloud SQL connectivity â†’ static external IP + allowlist
